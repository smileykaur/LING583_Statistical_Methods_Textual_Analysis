{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reddit analysis\n",
    "\n",
    "Keyword analysis of text that we previously collected from reddit (see 03-reddit_text.ipynb)\n",
    "\n",
    "Ling 583  \n",
    "15 Feb 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cytoolz import concat\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load comments from r/machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fb-brian</td>\n",
       "      <td>Hi everyone! We'd like to share a simple metho...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snendroid-ai</td>\n",
       "      <td>ads saying hot machine learning events near yo...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shadowalf</td>\n",
       "      <td>The only other thing I can think of is if ther...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nicolasap</td>\n",
       "      <td>It's open source and you can find it [here](ht...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ps2fats</td>\n",
       "      <td>Who has books anyway</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59042</th>\n",
       "      <td>dpineo</td>\n",
       "      <td>Start with CUDA.  It has better a much better ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59043</th>\n",
       "      <td>Optrode</td>\n",
       "      <td>What I was getting at is the problem of genera...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59044</th>\n",
       "      <td>Infidius</td>\n",
       "      <td>Most of our universe is limited in terms of no...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59045</th>\n",
       "      <td>hardmaru</td>\n",
       "      <td>It seems half of the dataset are images of art...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59046</th>\n",
       "      <td>hapliniste</td>\n",
       "      <td>I don't think we need anything like that for a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59047 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                               body  score\n",
       "0          fb-brian  Hi everyone! We'd like to share a simple metho...      2\n",
       "1      snendroid-ai  ads saying hot machine learning events near yo...      6\n",
       "2         shadowalf  The only other thing I can think of is if ther...      1\n",
       "3         nicolasap  It's open source and you can find it [here](ht...     36\n",
       "4           ps2fats                               Who has books anyway     67\n",
       "...             ...                                                ...    ...\n",
       "59042        dpineo  Start with CUDA.  It has better a much better ...      2\n",
       "59043       Optrode  What I was getting at is the problem of genera...      1\n",
       "59044      Infidius  Most of our universe is limited in terms of no...      2\n",
       "59045      hardmaru  It seems half of the dataset are images of art...      5\n",
       "59046    hapliniste  I don't think we need anything like that for a...      3\n",
       "\n",
       "[59047 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('machinelearning.csv', na_filter=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Find top 10 most prolific commenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "visarga                542\n",
       "ajmooch                435\n",
       "NicolasGuacamole       428\n",
       "alexmlamb              423\n",
       "epicwisdom             413\n",
       "darkconfidantislife    399\n",
       "bbsome                 375\n",
       "gwern                  346\n",
       "olBaa                  270\n",
       "phobrain               265\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = df.groupby('author')['author'].count().sort_values(ascending=False).head(10)\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Normalize and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['bow'] = df['body'].str.lower().str.replace(r'(\\W|\\d)+', ' ').str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Find keywords (by PMI) for each of the top 10 commenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visarga : starspace, simulators, relational, relations, eating, automata, fei, simulator, simulation, voices\n",
      "\n",
      "ajmooch : ajbrock, freezeout, batchrenorm, anneal, smashv, convs, bw, hypernet, crop, hyperparams\n",
      "\n",
      "NicolasGuacamole : generalisation, receptive, lasagne, learnmachinelearning, dilated, boosting, boundary, surely, semantic, insight\n",
      "\n",
      "alexmlamb : authorship, autoregressive, ali, alex, alignment, anonymous, epsilon, professors, forcing, icml\n",
      "\n",
      "epicwisdom : liberties, ontology, gmail, unethical, accused, sexism, likewise, emails, consciousness, conscious\n",
      "\n",
      "darkconfidantislife : ding, gabor, processors, googlenet, nm, processor, analog, chip, movement, convnets\n",
      "\n",
      "bbsome : gn, martens, h_min, hessian, ir, hf, llvm, nevertheless, kfac, additionally\n",
      "\n",
      "gwern : scott_e_reed, fredkin, webvision, kanal, danbooru, gwern, gaydar, tank, multilevel, mdps\n",
      "\n",
      "olBaa : qualia, adult, baby, intelligent, consciousness, defining, room, chinese, vec, turing\n",
      "\n",
      "phobrain : raf, skot, phobrain, gallery, ions, histograms, pics, cookie, dna, siamese\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def keywords(words):\n",
    "    f = pd.DataFrame({'all':pd.value_counts(list(concat(df[words])))})\n",
    "    for name in top10.index:\n",
    "        f['user'] = pd.value_counts(list(concat(df[df['author']==name][words])))\n",
    "        f['user_pmi'] = np.log2((f['user'] * np.sum(f['all'])) / \n",
    "                                (f['all'] * np.sum(f['user'])))\n",
    "        print(name, ':', ', '.join(f['user_pmi'][f['user']>5]\n",
    "                                       .sort_values(ascending=False)\n",
    "                                       .head(10)\n",
    "                                       .index))\n",
    "        print()\n",
    "keywords('bow')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "------\n",
    "\n",
    "## spaCy\n",
    "\n",
    "Now let's repeat that, but using the [spaCy](https://spacy.io/) tagger to find just nouns and then just verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a [Doc](https://spacy.io/api/doc) for each comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['doc'] = list(nlp.pipe(df['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we do the same thing as we did above, but this time our bag of words consists of the lemma forms of the nouns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visarga : simulator, consciousness, simulation, perception, relation, reasoning, robot, graph, processor, phrase\n",
      "\n",
      "ajmooch : convs, celeba, hypernet, guard, crop, sweep, speedup, densenet, hyperparam, batchnorm\n",
      "\n",
      "NicolasGuacamole : generalisation, heatmap, ensemble, being, estimation, room, luck, segmentation, insight, kernel\n",
      "\n",
      "alexmlamb : authorship, gaussian, professor, angle, chain, sampling, arxiv, connection, pass, credit\n",
      "\n",
      "epicwisdom : ontology, liberty, smartphone, sexism, consciousness, email, somebody, privacy, baby, distinction\n",
      "\n",
      "darkconfidantislife : processor, analog, chip, movement, being, imagenet, convnet, multiplication, array, precision\n",
      "\n",
      "bbsome : k[1, k[0, k[3, ps, autodiff, curvature, expansion, theta, delta, controller\n",
      "\n",
      "gwern : gaydar, tank, mdp, photograph, metadata, anime, stochasticity, glass, tag, causality\n",
      "\n",
      "olBaa : word2vec, baby, consciousness, room, projection, intelligence, definition, graph, dog, reviewer\n",
      "\n",
      "phobrain : ion, dna, pic, histogram, cookie, photo, taste, personality, pair, consistency\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_nouns(doc):\n",
    "    return [tok.lemma_ for tok in doc if tok.pos_ == 'NOUN']\n",
    "\n",
    "df['noun_bow'] = df['doc'].apply(get_nouns)\n",
    "\n",
    "keywords('noun_bow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And the same for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visarga : graph, compose, reuse, eat, operate, collect, generalize, search, extract, select\n",
      "\n",
      "ajmooch : anneal, scoop, dilate, freeze, roll, steal, concatenate, investigate, validate, pursue\n",
      "\n",
      "NicolasGuacamole : learnmachinelearn, dilate, â€™re, boost, â€™, drop, connect, agree, increase, say\n",
      "\n",
      "alexmlamb : inject, suspect, list, force, supervise, cite, connect, pick, push, regard\n",
      "\n",
      "epicwisdom : accuse, recall, count, recognize, die, disagree, justify, doubt, report, imagine\n",
      "\n",
      "darkconfidantislife : float, pass, hold, wait, suppose, compute, reduce, recommend, help, move\n",
      "\n",
      "bbsome : note, reuse, express, imply, care, suggest, claim, intend, agree, bind\n",
      "\n",
      "gwern : tag, infer, split, claim, demonstrate, supervise, list, discover, classify, notice\n",
      "\n",
      "olBaa : embed, define, gues, achieve, love, solve, believe, apply, happen, seem\n",
      "\n",
      "phobrain : eat, map, edit, match, explore, forget, figure, propose, could, develop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_verbs(doc):\n",
    "    return [tok.lemma_ for tok in doc if tok.pos_ == 'VERB']\n",
    "\n",
    "df['verb_bow'] = df['doc'].apply(get_verbs)\n",
    "\n",
    "keywords('verb_bow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
