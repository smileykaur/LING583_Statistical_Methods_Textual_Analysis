{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twenty_train=fetch_20newsgroups(subset='train', shuffle=True , random_state=42, categories=categories )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['description', 'DESCR', 'filenames', 'target_names', 'data', 'target']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predicting flight delays [Tutorial ]\n",
    "Fabien Daniel (September 2017)\n",
    "\n",
    "In this notebook, I develop a model aimed at predicting flight delays at take-off. The purpose is not to obtain the best possible prediction but rather to emphasize on the various steps needed to build such a model. Along this path, I then put in evidence some basic but important concepts. Among then, I comment on the importance of the separation of the dataset during the traning stage and how cross-validation helps in determing accurate model parameters. I show how to build linear and polynomial models for univariate or multivariate regressions and also, I give some insight on the reason why regularisation helps us in developing models that generalize well.\n",
    "\n",
    "From a technical point of view, the main aspects of python covered throughout the notebook are:\n",
    "\n",
    "visualization: matplolib, seaborn, basemap\n",
    "data manipulation: pandas, numpy\n",
    "modeling: sklearn, scipy\n",
    "class definition: regression, figures\n",
    "During the EDA, I intended to create good quality figures from which the information would be easily accessible at a first glance. An important aspect of the data scientist job consists in divulgating its findings to people who do not necessarily have knowledge in the technical aspects data scientists master. Graphics are surely the most powerful tool to achieve that goal, and mastering visualization techniques thus seems important.\n",
    "\n",
    "Also, as soon as an action is repeated (mostly at identical) a few times, I tend to write classes or functions and eventually embed them in loops. Doing so is sometimes longer than a simple copy-paste-edit process but, on the one hand, this improves the readibility of the code and most importantly, this reduces the number of lines of code (and so, the number of opportunities to introduce mistakes !!). In the current notebook, I defined classes in the modeling part in order to perform regressions. I also defined a class to wrap the making of figures. This allows to create stylish figures, by tuning the matplotlib parameters, that can be subsequently re-used thanks to that template. I feel that this could be useful to create nice looking graphics and then use them extensively once you are satisfied with the tuning. Moreover, this helps to keep some homogeneity in your plots.\n",
    "\n",
    "Acknowledgement: many thanks to J. Abécassis for the advices and help provided during the writing of this notebook \n",
    "\n",
    "This notebook is composed of three parts: cleaning (section 1), exploration (section 2-5) and modeling (section 6).\n",
    "\n",
    "Preamble: overview of the dataset \n",
    "\n",
    "1. Cleaning\n",
    "\n",
    "1.1 Dates and times\n",
    "1.2 Filling factor\n",
    "2. Comparing airlines\n",
    "\n",
    "2.1 Basic statistical description of airlines\n",
    "2.2 Delays distribution: establishing the ranking of airlines\n",
    "3. Delays: take-off or landing ? \n",
    "4. Relation between the origin airport and delays \n",
    "\n",
    "4.1 Geographical area covered by airlines \n",
    "4.2 How the origin airport impact delays \n",
    "4.3 Flights with usual delays ? \n",
    "5. Temporal variability of delays \n",
    "6. Predicting flight delays \n",
    "\n",
    "6.1 Model nº1: one airline, one airport\n",
    "6.1.1 Pitfalls\n",
    "6.1.2 Polynomial degree: splitting the dataset\n",
    "6.1.3 Model test: prediction of end-January delays\n",
    "6.2 Model nº2: one airline, all airports\n",
    "6.2.1 Linear regression\n",
    "6.2.2 Polynomial regression\n",
    "6.2.3 Setting the free parameters\n",
    "6.2.4 Model test: prediction of end-January delays\n",
    "6.3 Model nº3: Accounting for destinations\n",
    "6.3.1 Choice of the free parameters\n",
    "6.3.2 Model test: prediction of end-January delays\n",
    "Conclusion\n",
    "\n",
    "Preamble: overview of the dataset\n",
    "First, I load all the packages that will be needed during this project:\n",
    "\n",
    "import datetime, warnings, scipy \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from collections import OrderedDict\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from sklearn import metrics, linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from scipy.optimize import curve_fit\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "plt.style.use('fivethirtyeight')\n",
    "mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "pd.options.display.max_columns = 50\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "and then, I read the file that contains the details of all the flights that occured in 2015. I output some informations concerning the types of the variables in the dataframe and the quantity of null values for each variable:\n",
    "\n",
    "df = pd.read_csv('../input/flights.csv', low_memory=False)\n",
    "print('Dataframe dimensions:', df.shape)\n",
    "#____________________________________________________________\n",
    "# gives some infos on columns types and number of null values\n",
    "tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\n",
    "tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
    "tab_info=tab_info.append(pd.DataFrame(df.isnull().sum()/df.shape[0]*100)\n",
    "                         .T.rename(index={0:'null values (%)'}))\n",
    "tab_info\n",
    "Dataframe dimensions: (5819079, 31)\n",
    "YEAR\tMONTH\tDAY\tDAY_OF_WEEK\tAIRLINE\tFLIGHT_NUMBER\tTAIL_NUMBER\tORIGIN_AIRPORT\tDESTINATION_AIRPORT\tSCHEDULED_DEPARTURE\tDEPARTURE_TIME\tDEPARTURE_DELAY\tTAXI_OUT\tWHEELS_OFF\tSCHEDULED_TIME\tELAPSED_TIME\tAIR_TIME\tDISTANCE\tWHEELS_ON\tTAXI_IN\tSCHEDULED_ARRIVAL\tARRIVAL_TIME\tARRIVAL_DELAY\tDIVERTED\tCANCELLED\tCANCELLATION_REASON\tAIR_SYSTEM_DELAY\tSECURITY_DELAY\tAIRLINE_DELAY\tLATE_AIRCRAFT_DELAY\tWEATHER_DELAY\n",
    "column type\tint64\tint64\tint64\tint64\tobject\tint64\tobject\tobject\tobject\tint64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tint64\tfloat64\tfloat64\tint64\tint64\tobject\tfloat64\tfloat64\tfloat64\tfloat64\tfloat64\n",
    "null values (nb)\t0\t0\t0\t0\t0\t0\t14721\t0\t0\t0\t86153\t86153\t89047\t89047\t6\t105071\t105071\t0\t92513\t92513\t0\t92513\t105071\t0\t0\t5729195\t4755640\t4755640\t4755640\t4755640\t4755640\n",
    "null values (%)\t0\t0\t0\t0\t0\t0\t0.252978\t0\t0\t0\t1.48053\t1.48053\t1.53026\t1.53026\t0.000103109\t1.80563\t1.80563\t0\t1.58982\t1.58982\t0\t1.58982\t1.80563\t0\t0\t98.4554\t81.725\t81.725\t81.725\t81.725\t81.725\n",
    "Each entry of the flights.csv file corresponds to a flight and we see that more than 5'800'000 flights have been recorded in 2015. These flights are described according to 31 variables. A description of these variables can be found here and I briefly recall the meaning of the variables that will be used in this notebook:\n",
    "\n",
    "YEAR, MONTH, DAY, DAY_OF_WEEK: dates of the flight \n",
    "AIRLINE: An identification number assigned by US DOT to identify a unique airline \n",
    "ORIGIN_AIRPORT and DESTINATION_AIRPORT: code attributed by IATA to identify the airports \n",
    "SCHEDULED_DEPARTURE and SCHEDULED_ARRIVAL : scheduled times of take-off and landing \n",
    "DEPARTURE_TIME and ARRIVAL_TIME: real times at which take-off and landing took place \n",
    "DEPARTURE_DELAY and ARRIVAL_DELAY: difference (in minutes) between planned and real times \n",
    "DISTANCE: distance (in miles) \n",
    "An additional file of this dataset, the airports.csv file, gives a more exhaustive description of the airports:\n",
    "\n",
    "airports = pd.read_csv(\"../input/airports.csv\")\n",
    "To have a global overview of the geographical area covered in this dataset, we can plot the airports location and indicate the number of flights recorded during year 2015 in each of them:\n",
    "\n",
    "count_flights = df['ORIGIN_AIRPORT'].value_counts()\n",
    "#___________________________\n",
    "plt.figure(figsize=(11,11))\n",
    "#________________________________________\n",
    "# define properties of markers and labels\n",
    "colors = ['yellow', 'red', 'lightblue', 'purple', 'green', 'orange']\n",
    "size_limits = [1, 100, 1000, 10000, 100000, 1000000]\n",
    "labels = []\n",
    "for i in range(len(size_limits)-1):\n",
    "    labels.append(\"{} <.< {}\".format(size_limits[i], size_limits[i+1])) \n",
    "#____________________________________________________________\n",
    "map = Basemap(resolution='i',llcrnrlon=-180, urcrnrlon=-50,\n",
    "              llcrnrlat=10, urcrnrlat=75, lat_0=0, lon_0=0,)\n",
    "map.shadedrelief()\n",
    "map.drawcoastlines()\n",
    "map.drawcountries(linewidth = 3)\n",
    "map.drawstates(color='0.3')\n",
    "#_____________________\n",
    "# put airports on map\n",
    "for index, (code, y,x) in airports[['IATA_CODE', 'LATITUDE', 'LONGITUDE']].iterrows():\n",
    "    x, y = map(x, y)\n",
    "    isize = [i for i, val in enumerate(size_limits) if val < count_flights[code]]\n",
    "    ind = isize[-1]\n",
    "    map.plot(x, y, marker='o', markersize = ind+5, markeredgewidth = 1, color = colors[ind],\n",
    "             markeredgecolor='k', label = labels[ind])\n",
    "#_____________________________________________\n",
    "# remove duplicate labels and set their order\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = OrderedDict(zip(labels, handles))\n",
    "key_order = ('1 <.< 100', '100 <.< 1000', '1000 <.< 10000',\n",
    "             '10000 <.< 100000', '100000 <.< 1000000')\n",
    "new_label = OrderedDict()\n",
    "for key in key_order:\n",
    "    new_label[key] = by_label[key]\n",
    "plt.legend(new_label.values(), new_label.keys(), loc = 1, prop= {'size':11},\n",
    "           title='Number of flights per year', frameon = True, framealpha = 1)\n",
    "plt.show()\n",
    "\n",
    "Given the large size of the dataset, I decide to consider only a subset of the data in order to reduce the computational time. I will just keep the flights from January 2015:\n",
    "\n",
    "df = df[df['MONTH'] == 1]\n",
    "1. Cleaning\n",
    "1.1 Dates and times\n",
    "In the initial dataframe, dates are coded according to 4 variables: YEAR, MONTH, DAY, and DAY_OF_WEEK. In fact, python offers the datetime format which is really convenient to work with dates and times and I thus convert the dates in this format:\n",
    "\n",
    "df['DATE'] = pd.to_datetime(df[['YEAR','MONTH', 'DAY']])\n",
    "Moreover, in the SCHEDULED_DEPARTURE variable, the hour of the take-off is coded as a float where the two first digits indicate the hour and the two last, the minutes. This format is not convenient and I thus convert it. Finally, I merge the take-off hour with the flight date. To proceed with these transformations, I define a few functions:\n",
    "\n",
    "#_________________________________________________________\n",
    "# Function that convert the 'HHMM' string to datetime.time\n",
    "def format_heure(chaine):\n",
    "    if pd.isnull(chaine):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if chaine == 2400: chaine = 0\n",
    "        chaine = \"{0:04d}\".format(int(chaine))\n",
    "        heure = datetime.time(int(chaine[0:2]), int(chaine[2:4]))\n",
    "        return heure\n",
    "#_____________________________________________________________________\n",
    "# Function that combines a date and time to produce a datetime.datetime\n",
    "def combine_date_heure(x):\n",
    "    if pd.isnull(x[0]) or pd.isnull(x[1]):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return datetime.datetime.combine(x[0],x[1])\n",
    "#_______________________________________________________________________________\n",
    "# Function that combine two columns of the dataframe to create a datetime format\n",
    "def create_flight_time(df, col):    \n",
    "    liste = []\n",
    "    for index, cols in df[['DATE', col]].iterrows():    \n",
    "        if pd.isnull(cols[1]):\n",
    "            liste.append(np.nan)\n",
    "        elif float(cols[1]) == 2400:\n",
    "            cols[0] += datetime.timedelta(days=1)\n",
    "            cols[1] = datetime.time(0,0)\n",
    "            liste.append(combine_date_heure(cols))\n",
    "        else:\n",
    "            cols[1] = format_heure(cols[1])\n",
    "            liste.append(combine_date_heure(cols))\n",
    "    return pd.Series(liste)\n",
    "and I call them to modify the dataframe variables:\n",
    "\n",
    "df['SCHEDULED_DEPARTURE'] = create_flight_time(df, 'SCHEDULED_DEPARTURE')\n",
    "df['DEPARTURE_TIME'] = df['DEPARTURE_TIME'].apply(format_heure)\n",
    "df['SCHEDULED_ARRIVAL'] = df['SCHEDULED_ARRIVAL'].apply(format_heure)\n",
    "df['ARRIVAL_TIME'] = df['ARRIVAL_TIME'].apply(format_heure)\n",
    "#__________________________________________________________________________\n",
    "df.loc[:5, ['SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL', 'DEPARTURE_TIME',\n",
    "             'ARRIVAL_TIME', 'DEPARTURE_DELAY', 'ARRIVAL_DELAY']]\n",
    "SCHEDULED_DEPARTURE\tSCHEDULED_ARRIVAL\tDEPARTURE_TIME\tARRIVAL_TIME\tDEPARTURE_DELAY\tARRIVAL_DELAY\n",
    "0\t2015-01-01 00:05:00\t04:30:00\t23:54:00\t04:08:00\t-11.0\t-22.0\n",
    "1\t2015-01-01 00:10:00\t07:50:00\t00:02:00\t07:41:00\t-8.0\t-9.0\n",
    "2\t2015-01-01 00:20:00\t08:06:00\t00:18:00\t08:11:00\t-2.0\t5.0\n",
    "3\t2015-01-01 00:20:00\t08:05:00\t00:15:00\t07:56:00\t-5.0\t-9.0\n",
    "4\t2015-01-01 00:25:00\t03:20:00\t00:24:00\t02:59:00\t-1.0\t-21.0\n",
    "5\t2015-01-01 00:25:00\t06:02:00\t00:20:00\t06:10:00\t-5.0\t8.0\n",
    "Note that in practice, the content of the DEPARTURE_TIME and ARRIVAL_TIME variables can be a bit misleading since they don't contain the dates. For exemple, in the first entry of the dataframe, the scheduled departure is at 0h05 the 1st of January. The DEPARTURE_TIME variable indicates 23h54 and we thus don't know if the flight leaved before time or if there was a large delay. Hence, the DEPARTURE_DELAY and ARRIVAL_DELAY variables proves more useful since they directly provides the delays in minutes. Hence, in what follows, I will not use the DEPARTURE_TIME and ARRIVAL_TIME variables.\n",
    "\n",
    "1.2 Filling factor\n",
    "Finally, I clean the dataframe throwing the variables I won't use and re-organize the columns to ease its reading:\n",
    "\n",
    "variables_to_remove = ['TAXI_OUT', 'TAXI_IN', 'WHEELS_ON', 'WHEELS_OFF', 'YEAR', \n",
    "                       'MONTH','DAY','DAY_OF_WEEK','DATE', 'AIR_SYSTEM_DELAY',\n",
    "                       'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY',\n",
    "                       'WEATHER_DELAY', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON',\n",
    "                       'FLIGHT_NUMBER', 'TAIL_NUMBER', 'AIR_TIME']\n",
    "df.drop(variables_to_remove, axis = 1, inplace = True)\n",
    "df = df[['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n",
    "        'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'DEPARTURE_DELAY',\n",
    "        'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY',\n",
    "        'SCHEDULED_TIME', 'ELAPSED_TIME']]\n",
    "df[:5]\n",
    "AIRLINE\tORIGIN_AIRPORT\tDESTINATION_AIRPORT\tSCHEDULED_DEPARTURE\tDEPARTURE_TIME\tDEPARTURE_DELAY\tSCHEDULED_ARRIVAL\tARRIVAL_TIME\tARRIVAL_DELAY\tSCHEDULED_TIME\tELAPSED_TIME\n",
    "0\tAS\tANC\tSEA\t2015-01-01 00:05:00\t23:54:00\t-11.0\t04:30:00\t04:08:00\t-22.0\t205.0\t194.0\n",
    "1\tAA\tLAX\tPBI\t2015-01-01 00:10:00\t00:02:00\t-8.0\t07:50:00\t07:41:00\t-9.0\t280.0\t279.0\n",
    "2\tUS\tSFO\tCLT\t2015-01-01 00:20:00\t00:18:00\t-2.0\t08:06:00\t08:11:00\t5.0\t286.0\t293.0\n",
    "3\tAA\tLAX\tMIA\t2015-01-01 00:20:00\t00:15:00\t-5.0\t08:05:00\t07:56:00\t-9.0\t285.0\t281.0\n",
    "4\tAS\tSEA\tANC\t2015-01-01 00:25:00\t00:24:00\t-1.0\t03:20:00\t02:59:00\t-21.0\t235.0\t215.0\n",
    "At this stage, I examine how complete the dataset is:\n",
    "\n",
    "variable\tmissing values\tfilling factor (%)\n",
    "0\tARRIVAL_DELAY\t12955\t97.243429\n",
    "1\tELAPSED_TIME\t12955\t97.243429\n",
    "2\tARRIVAL_TIME\t12271\t97.388971\n",
    "3\tDEPARTURE_TIME\t11657\t97.519618\n",
    "4\tDEPARTURE_DELAY\t11657\t97.519618\n",
    "5\tAIRLINE\t0\t100.000000\n",
    "6\tORIGIN_AIRPORT\t0\t100.000000\n",
    "7\tDESTINATION_AIRPORT\t0\t100.000000\n",
    "8\tSCHEDULED_DEPARTURE\t0\t100.000000\n",
    "9\tSCHEDULED_ARRIVAL\t0\t100.000000\n",
    "10\tSCHEDULED_TIME\t0\t100.000000\n",
    "We see that the variables filling factor is quite good (> 97%). Since the scope of this work is not to establish the state-of-the-art in predicting flight delays, I decide to proceed without trying to impute what's missing and I simply remove the entries that contain missing values.\n",
    "\n",
    "2. Comparing airlines\n",
    "As said earlier, the AIRLINE variable contains the airline abreviations. Their full names can be retrieved from the airlines.csv file.\n",
    "\n",
    "airlines_names = pd.read_csv('../input/airlines.csv')\n",
    "airlines_names\n",
    "IATA_CODE\tAIRLINE\n",
    "0\tUA\tUnited Air Lines Inc.\n",
    "1\tAA\tAmerican Airlines Inc.\n",
    "2\tUS\tUS Airways Inc.\n",
    "3\tF9\tFrontier Airlines Inc.\n",
    "4\tB6\tJetBlue Airways\n",
    "5\tOO\tSkywest Airlines Inc.\n",
    "6\tAS\tAlaska Airlines Inc.\n",
    "7\tNK\tSpirit Air Lines\n",
    "8\tWN\tSouthwest Airlines Co.\n",
    "9\tDL\tDelta Air Lines Inc.\n",
    "10\tEV\tAtlantic Southeast Airlines\n",
    "11\tHA\tHawaiian Airlines Inc.\n",
    "12\tMQ\tAmerican Eagle Airlines Inc.\n",
    "13\tVX\tVirgin America\n",
    "For further use, I put the content of this this dataframe in a dictionary:\n",
    "\n",
    "abbr_companies = airlines_names.set_index('IATA_CODE')['AIRLINE'].to_dict()\n",
    "2.1 Basic statistical description of airlines\n",
    "As a first step, I consider all the flights from all carriers. Here, the aim is to classify the airlines with respect to their punctuality and for that purpose, I compute a few basic statisticial parameters:\n",
    "\n",
    "#__________________________________________________________________\n",
    "# function that extract statistical parameters from a grouby objet:\n",
    "def get_stats(group):\n",
    "    return {'min': group.min(), 'max': group.max(),\n",
    "            'count': group.count(), 'mean': group.mean()}\n",
    "#_______________________________________________________________\n",
    "# Creation of a dataframe with statitical infos on each airline:\n",
    "global_stats = df['DEPARTURE_DELAY'].groupby(df['AIRLINE']).apply(get_stats).unstack()\n",
    "global_stats = global_stats.sort_values('count')\n",
    "global_stats\n",
    "count\tmax\tmean\tmin\n",
    "AIRLINE\t\t\t\t\n",
    "VX\t4647.0\t397.0\t6.896277\t-20.0\n",
    "HA\t6408.0\t1003.0\t1.311954\t-26.0\n",
    "F9\t6735.0\t696.0\t17.910765\t-32.0\n",
    "NK\t8632.0\t557.0\t13.073100\t-28.0\n",
    "AS\t13151.0\t444.0\t3.072086\t-47.0\n",
    "B6\t20482.0\t500.0\t9.988331\t-27.0\n",
    "MQ\t27568.0\t780.0\t15.995865\t-29.0\n",
    "US\t32478.0\t638.0\t5.175011\t-26.0\n",
    "UA\t37363.0\t886.0\t13.885555\t-40.0\n",
    "AA\t43074.0\t1988.0\t10.548335\t-29.0\n",
    "OO\t46655.0\t931.0\t11.999957\t-48.0\n",
    "EV\t48084.0\t726.0\t9.678895\t-33.0\n",
    "DL\t63676.0\t1184.0\t5.888215\t-26.0\n",
    "WN\t98060.0\t604.0\t9.453426\t-15.0\n",
    "Now, in order to facilitate the lecture of that information, I construct some graphics:\n",
    "\n",
    "font = {'family' : 'normal', 'weight' : 'bold', 'size'   : 15}\n",
    "mpl.rc('font', **font)\n",
    "import matplotlib.patches as mpatches\n",
    "#__________________________________________________________________\n",
    "# I extract a subset of columns and redefine the airlines labeling \n",
    "df2 = df.loc[:, ['AIRLINE', 'DEPARTURE_DELAY']]\n",
    "df2['AIRLINE'] = df2['AIRLINE'].replace(abbr_companies)\n",
    "#________________________________________________________________________\n",
    "colors = ['royalblue', 'grey', 'wheat', 'c', 'firebrick', 'seagreen', 'lightskyblue',\n",
    "          'lightcoral', 'yellowgreen', 'gold', 'tomato', 'violet', 'aquamarine', 'chartreuse']\n",
    "#___________________________________\n",
    "fig = plt.figure(1, figsize=(16,15))\n",
    "gs=GridSpec(2,2)             \n",
    "ax1=fig.add_subplot(gs[0,0]) \n",
    "ax2=fig.add_subplot(gs[0,1]) \n",
    "ax3=fig.add_subplot(gs[1,:]) \n",
    "#------------------------------\n",
    "# Pie chart nº1: nb of flights\n",
    "#------------------------------\n",
    "labels = [s for s in  global_stats.index]\n",
    "sizes  = global_stats['count'].values\n",
    "explode = [0.3 if sizes[i] < 20000 else 0.0 for i in range(len(abbr_companies))]\n",
    "patches, texts, autotexts = ax1.pie(sizes, explode = explode,\n",
    "                                labels=labels, colors = colors,  autopct='%1.0f%%',\n",
    "                                shadow=False, startangle=0)\n",
    "for i in range(len(abbr_companies)): \n",
    "    texts[i].set_fontsize(14)\n",
    "ax1.axis('equal')\n",
    "ax1.set_title('% of flights per company', bbox={'facecolor':'midnightblue', 'pad':5},\n",
    "              color = 'w',fontsize=18)\n",
    "#_______________________________________________\n",
    "# I set the legend: abreviation -> airline name\n",
    "comp_handler = []\n",
    "for i in range(len(abbr_companies)):\n",
    "    comp_handler.append(mpatches.Patch(color=colors[i],\n",
    "            label = global_stats.index[i] + ': ' + abbr_companies[global_stats.index[i]]))\n",
    "ax1.legend(handles=comp_handler, bbox_to_anchor=(0.2, 0.9), \n",
    "           fontsize = 13, bbox_transform=plt.gcf().transFigure)\n",
    "#----------------------------------------\n",
    "# Pie chart nº2: mean delay at departure\n",
    "#----------------------------------------\n",
    "sizes  = global_stats['mean'].values\n",
    "sizes  = [max(s,0) for s in sizes]\n",
    "explode = [0.0 if sizes[i] < 20000 else 0.01 for i in range(len(abbr_companies))]\n",
    "patches, texts, autotexts = ax2.pie(sizes, explode = explode, labels = labels,\n",
    "                                colors = colors, shadow=False, startangle=0,\n",
    "                                autopct = lambda p :  '{:.0f}'.format(p * sum(sizes) / 100))\n",
    "for i in range(len(abbr_companies)): \n",
    "    texts[i].set_fontsize(14)\n",
    "ax2.axis('equal')\n",
    "ax2.set_title('Mean delay at origin', bbox={'facecolor':'midnightblue', 'pad':5},\n",
    "              color='w', fontsize=18)\n",
    "#------------------------------------------------------\n",
    "# striplot with all the values reported for the delays\n",
    "#___________________________________________________________________\n",
    "# I redefine the colors for correspondance with the pie charts\n",
    "colors = ['firebrick', 'gold', 'lightcoral', 'aquamarine', 'c', 'yellowgreen', 'grey',\n",
    "          'seagreen', 'tomato', 'violet', 'wheat', 'chartreuse', 'lightskyblue', 'royalblue']\n",
    "#___________________________________________________________________\n",
    "ax3 = sns.stripplot(y=\"AIRLINE\", x=\"DEPARTURE_DELAY\", size = 4, palette = colors,\n",
    "                    data=df2, linewidth = 0.5,  jitter=True)\n",
    "plt.setp(ax3.get_xticklabels(), fontsize=14)\n",
    "plt.setp(ax3.get_yticklabels(), fontsize=14)\n",
    "ax3.set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*[int(y) for y in divmod(x,60)])\n",
    "                         for x in ax3.get_xticks()])\n",
    "plt.xlabel('Departure delay', fontsize=18, bbox={'facecolor':'midnightblue', 'pad':5},\n",
    "           color='w', labelpad=20)\n",
    "ax3.yaxis.label.set_visible(False)\n",
    "#________________________\n",
    "plt.tight_layout(w_pad=3) \n",
    "\n",
    "Considering the first pie chart that gives the percentage of flights per airline, we see that there is some disparity between the carriers. For exemple, Southwest Airlines accounts for  ∼ 20% of the flights which is similar to the number of flights chartered by the 7 tiniest airlines. However, if we have a look at the second pie chart, we see that here, on the contrary, the differences among airlines are less pronounced. Excluding Hawaiian Airlines and Alaska Airlines that report extremely low mean delays, we obtain that a value of  ∼ 11 ± 7 minutes would correctly represent all mean delays. Note that this value is quite low which mean that the standard for every airline is to respect the schedule !\n",
    "\n",
    "Finally, the figure at the bottom makes a census of all the delays that were measured in January 2015. This representation gives a feeling on the dispersion of data and put in perspective the relative homogeneity that appeared in the second pie chart. Indeed, we see that while all mean delays are around 10 minutes, this low value is a consequence of the fact that a majority of flights take off on time. However, we see that occasionally, we can face really large delays that can reach a few tens of hours !\n",
    "\n",
    "The large majority of short delays is visible in the next figure:\n",
    "\n",
    "\n",
    "This figure gives a count of the delays of less than 5 minutes, those in the range 5 < t < 45 min and finally, the delays greater than 45 minutes. Hence, we wee that independently of the airline, delays greater than 45 minutes only account for a few percents. However, the proportion of delays in these three groups depends on the airline: as an exemple, in the case of SkyWest Airlines, the delays greater than 45 minutes are only lower by  ∼ 30% with respect to delays in the range 5 < t < 45 min. Things are better for SoutWest Airlines since delays greater than 45 minutes are 4 times less frequent than delays in the range 5 < t < 45 min.\n",
    "\n",
    "2.2 Delays distribution: establishing the ranking of airlines\n",
    "It was shown in the previous section that mean delays behave homogeneously among airlines (apart from two extrem cases) and is around 11 ± 7 minutes. Then, we saw that this low value is a consequence of the large proportion of flights that take off on time. However, occasionally, large delays can be registred. In this section, I examine more in details the distribution of delays for every airlines:\n",
    "\n",
    "\n",
    "This figure shows the normalised distribution of delays that I modelised with an exponential distribution  f(x)=aexp(−x/b) . The  a  et  b  parameters obtained to describe each airline are given in the upper right corner of each panel. Note that the normalisation of the distribution implies that  ∫f(x)dx∼1 . Here, we do not have a strict equality since the normalisation applies the histograms but not to the model function. However, this relation entails that the  a  et  b  coefficients will be correlated with  a∝1/b  and hence, only one of these two values is necessary to describe the distributions. Finally, according to the value of either  a  or  b , it is possible to establish a ranking of the companies: the low values of  a  will correspond to airlines with a large proportion of important delays and, on the contrary, airlines that shine from their punctuality will admit hight  a  values:\n",
    "\n",
    "\n",
    "The left panel of this figure gives an overview of the  a  and  b  coefficients of the 14 airlines showing that Hawaiian Airlines and Delta Airlines occupy the first two places. The right panel zooms on 12 other airlines. We can see that SouthWest Airlines, which represent  ∼ 20% of the total number of flights is well ranked and occupy the third position. According to this ranking, SkyWest Airlines is the worst carrier.\n",
    "\n",
    "3. Delays: take-off or landing ?\n",
    "In the previous section, all the discussion was done on departure delays. However, these delays differ somewhat from the delays recorded at arrival:\n",
    "\n",
    "\n",
    "On this figure, we can see that delays at arrival are generally lower than at departure. This indicates that airlines adjust their flight speed in order to reduce the delays at arrival. In what follows, I will just consider the delays at departure but one has to keep in mind that this can differ from arrival delays.\n",
    "\n",
    "4. Relation between the origin airport and delays\n",
    "I will now try to define if there is a correlation between the delays registered and the airport of origin. I recall that in the dataset, the number of airports considered is:\n",
    "\n",
    "Nb of airports: 312\n",
    "4.1 Geographical area covered by airlines\n",
    "Here, I have a quick look at the number of destination airports for each airline:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4.2 How the origin airport impact delays\n",
    "In this section, I will have a look at the variations of the delays with respect to the origin airport and for every airline. The first step thus consists in determining the mean delays per airport:\n",
    "\n",
    "airport_mean_delays = pd.DataFrame(pd.Series(df['ORIGIN_AIRPORT'].unique()))\n",
    "airport_mean_delays.set_index(0, drop = True, inplace = True)\n",
    "\n",
    "for carrier in abbr_companies.keys():\n",
    "    df1 = df[df['AIRLINE'] == carrier]\n",
    "    test = df1['DEPARTURE_DELAY'].groupby(df['ORIGIN_AIRPORT']).apply(get_stats).unstack()\n",
    "    airport_mean_delays[carrier] = test.loc[:, 'mean'] \n",
    "Since the number of airports is quite large, a graph showing all the information at once would be a bit messy, since it would represent around 4400 values (i.e. 312 airports  ×  14 airlines). Hence, I just represent a subset of the data:\n",
    "\n",
    "\n",
    "This figure allows to draw some conclusions. First, by looking at the data associated with the different airlines, we find the behavior we previously observed: for example, if we consider the right panel, it will be seen that the column associated with American Eagle Airlines mostly reports large delays, while the column associated with Delta Airlines is mainly associated with delays of less than 5 minutes. If we now look at the airports of origin, we will see that some airports favor late departures: see e.g. Denver, Chicago or New York. Conversely, other airports will mainly know on time departures such as Portland or Oakland.\n",
    "\n",
    "Finally, we can deduce from these observations that there is a high variability in average delays, both between the different airports but also between the different airlines. This is important because it implies that in order to accurately model the delays, it will be necessary to adopt a model that is specific to the company and the home airport .\n",
    "\n",
    "4.3 Flights with usual delays ?\n",
    "In the previous section, it has been seen that there is variability in delays when considering the different airlines and the different airports of origin. I'm now going to add a level of granularity by focusing not just on the original airports but on flights: origin  →  destination. The objective here is to see if some flights are systematically delayed or if, on the contrary, there are flights that would always be on time.\n",
    "\n",
    "In the following, I consider the case of a single airline. I list all the flights A  →  B carried out by this company and for each of them, I create the list of delays that have been recorded:\n",
    "\n",
    "#_________________________________________________________________\n",
    "# We select the company and create a subset of the main dataframe\n",
    "carrier = 'AA'\n",
    "df1 = df[df['AIRLINE']==carrier][['ORIGIN_AIRPORT','DESTINATION_AIRPORT','DEPARTURE_DELAY']]\n",
    "#___________________________________________________________\n",
    "# I collect the routes and list the delays for each of them\n",
    "trajet = dict()\n",
    "for ind, col in df1.iterrows():\n",
    "    if pd.isnull(col['DEPARTURE_DELAY']): continue\n",
    "    route = str(col['ORIGIN_AIRPORT'])+'-'+str(col['DESTINATION_AIRPORT'])\n",
    "    if route in trajet.keys():\n",
    "        trajet[route].append(col['DEPARTURE_DELAY'])\n",
    "    else:\n",
    "        trajet[route] = [col['DEPARTURE_DELAY']]\n",
    "#____________________________________________________________________        \n",
    "# I transpose the dictionary in a list to sort the routes by origins        \n",
    "liste_trajet = []\n",
    "for key, value in trajet.items():\n",
    "    liste_trajet.append([key, value])\n",
    "liste_trajet.sort()\n",
    "I then calculate the average delay on the various paths A  →  B, as well as the standard deviation and once done, I create a graphical representation (for a sample of the flights):\n",
    "\n",
    "mean_val = [] ; std_val = [] ; x_label = []\n",
    "\n",
    "i = 0\n",
    "for route, liste_retards in liste_trajet:\n",
    "    #_____________________________________________\n",
    "    # I set the labels as the airport from origin\n",
    "    index = route.split('-')[0]\n",
    "    x_label.append(identify_airport[index])\n",
    "    #______________________________________________________________________________\n",
    "    # I put a threshold on delays to prevent that high values take too much weight\n",
    "    trajet2 = [min(90, s) for s in liste_retards]\n",
    "    #________________________________________\n",
    "    # I compute mean and standard deviations\n",
    "    mean_val.append(scipy.mean(trajet2))\n",
    "    std_val.append(scipy.std(trajet2))\n",
    "    i += 1\n",
    "#________________\n",
    "# Plot the graph\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "std_min = [ min(15 + mean_val[i], s) for i,s in enumerate(std_val)] \n",
    "ax.errorbar(list(range(i)), mean_val, yerr = [std_min, std_val], fmt='o') \n",
    "ax.set_title('Mean route delays for \"{}\"'.format(abbr_companies[carrier]),\n",
    "             fontsize=14, weight = 'bold')\n",
    "plt.ylabel('Mean delay at origin (minutes)', fontsize=14, weight = 'bold', labelpad=12)\n",
    "#___________________________________________________\n",
    "# I define the x,y range and positions of the ticks\n",
    "imin, imax = 145, 230\n",
    "plt.xlim(imin, imax) ; plt.ylim(-20, 45)\n",
    "liste_ticks = [imin]\n",
    "for j in range(imin+1,imax):\n",
    "    if x_label[j] == x_label[j-1]: continue\n",
    "    liste_ticks.append(j)\n",
    "#_____________________________\n",
    "# and set the tick parameters  \n",
    "ax.set_xticks(liste_ticks)\n",
    "ax.set_xticklabels([x_label[int(x)] for x in ax.get_xticks()], rotation = 90, fontsize = 8)\n",
    "plt.setp(ax.get_yticklabels(), fontsize=12, rotation = 0)\n",
    "ax.tick_params(axis='y', which='major', pad=15)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "This figure gives the average delays for American Airlines, according to the city of origin and the destination (note that on the abscissa axis, only the origin is indicated for the sake of clarity). The error bars associated with the different paths correspond to the standard deviations. In this example, it can be seen that for a given airport of origin, delays will fluctuate depending on the destination. We see, for example, that here the greatest variations are obtained for New York or Miami where the initial average delays vary between 0 and  ∼ 20 minutes.\n",
    "\n",
    "4. Temporal variability of delays\n",
    "In this section, I look at the way delays vary with time. Considering the case of a specific airline and airport, delays can be easily represented by day and time (aside: before doing this, I define a class that I will use extensively in what follows to produce graphs):\n",
    "\n",
    "class Figure_style():\n",
    "    #_________________________________________________________________\n",
    "    def __init__(self, size_x = 11, size_y = 5, nrows = 1, ncols = 1):\n",
    "        sns.set_style(\"white\")\n",
    "        sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\n",
    "        self.fig, axs = plt.subplots(nrows = nrows, ncols = ncols, figsize=(size_x,size_y,))\n",
    "        #________________________________\n",
    "        # convert self.axs to 2D array\n",
    "        if nrows == 1 and ncols == 1:\n",
    "            self.axs = np.reshape(axs, (1, -1))\n",
    "        elif nrows == 1:\n",
    "            self.axs = np.reshape(axs, (1, -1))\n",
    "        elif ncols == 1:\n",
    "            self.axs = np.reshape(axs, (-1, 1))\n",
    "    #_____________________________\n",
    "    def pos_update(self, ix, iy):\n",
    "        self.ix, self.iy = ix, iy\n",
    "    #_______________\n",
    "    def style(self):\n",
    "        self.axs[self.ix, self.iy].spines['right'].set_visible(False)\n",
    "        self.axs[self.ix, self.iy].spines['top'].set_visible(False)\n",
    "        self.axs[self.ix, self.iy].yaxis.grid(color='lightgray', linestyle=':')\n",
    "        self.axs[self.ix, self.iy].xaxis.grid(color='lightgray', linestyle=':')\n",
    "        self.axs[self.ix, self.iy].tick_params(axis='both', which='major',\n",
    "                                               labelsize=10, size = 5)\n",
    "    #________________________________________\n",
    "    def draw_legend(self, location='upper right'):\n",
    "        legend = self.axs[self.ix, self.iy].legend(loc = location, shadow=True,\n",
    "                                        facecolor = 'g', frameon = True)\n",
    "        legend.get_frame().set_facecolor('whitesmoke')\n",
    "    #_________________________________________________________________________________\n",
    "    def cust_plot(self, x, y, color='b', linestyle='-', linewidth=1, marker=None, label=''):\n",
    "        if marker:\n",
    "            markerfacecolor, marker, markersize = marker[:]\n",
    "            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n",
    "                                linewidth = linewidth, marker = marker, label = label,\n",
    "                                markerfacecolor = markerfacecolor, markersize = markersize)\n",
    "        else:\n",
    "            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n",
    "                                        linewidth = linewidth, label=label)\n",
    "        self.fig.autofmt_xdate()\n",
    "    #________________________________________________________________________\n",
    "    def cust_plot_date(self, x, y, color='lightblue', linestyle='-',\n",
    "                       linewidth=1, markeredge=False, label=''):\n",
    "        markeredgewidth = 1 if markeredge else 0\n",
    "        self.axs[self.ix, self.iy].plot_date(x, y, color='lightblue', markeredgecolor='grey',\n",
    "                                  markeredgewidth = markeredgewidth, label=label)\n",
    "    #________________________________________________________________________\n",
    "    def cust_scatter(self, x, y, color = 'lightblue', markeredge = False, label=''):\n",
    "        markeredgewidth = 1 if markeredge else 0\n",
    "        self.axs[self.ix, self.iy].scatter(x, y, color=color,  edgecolor='grey',\n",
    "                                  linewidths = markeredgewidth, label=label)    \n",
    "    #___________________________________________\n",
    "    def set_xlabel(self, label, fontsize = 14):\n",
    "        self.axs[self.ix, self.iy].set_xlabel(label, fontsize = fontsize)\n",
    "    #___________________________________________\n",
    "    def set_ylabel(self, label, fontsize = 14):\n",
    "        self.axs[self.ix, self.iy].set_ylabel(label, fontsize = fontsize)\n",
    "    #____________________________________\n",
    "    def set_xlim(self, lim_inf, lim_sup):\n",
    "        self.axs[self.ix, self.iy].set_xlim([lim_inf, lim_sup])\n",
    "    #____________________________________\n",
    "    def set_ylim(self, lim_inf, lim_sup):\n",
    "        self.axs[self.ix, self.iy].set_ylim([lim_inf, lim_sup])           \n",
    "carrier = 'WN'\n",
    "id_airport = 4\n",
    "liste_origin_airport = df[df['AIRLINE'] == carrier]['ORIGIN_AIRPORT'].unique()\n",
    "df2 = df[(df['AIRLINE'] == carrier) & (df['ARRIVAL_DELAY'] > 0)\n",
    "         & (df['ORIGIN_AIRPORT'] == liste_origin_airport[id_airport])]\n",
    "df2.sort_values('SCHEDULED_DEPARTURE', inplace = True)\n",
    "fig1 = Figure_style(11, 5, 1, 1)\n",
    "fig1.pos_update(0, 0)\n",
    "fig1.cust_plot(df2['SCHEDULED_DEPARTURE'], df2['DEPARTURE_DELAY'], linestyle='-')\n",
    "fig1.style() \n",
    "fig1.set_ylabel('Delay (minutes)', fontsize = 14)\n",
    "fig1.set_xlabel('Departure date', fontsize = 14)\n",
    "date_1 = datetime.datetime(2015,1,1)\n",
    "date_2 = datetime.datetime(2015,1,15)\n",
    "fig1.set_xlim(date_1, date_2)\n",
    "fig1.set_ylim(-15, 260)\n",
    "\n",
    "This figure shows the existence of cycles, both in the frequency of the delays but also in their magnitude. In fact, intuitively, it seems quite logical to observe such cycles since they will be a consequence of the day-night alternation and the fact that the airport activity will be greatly reduced (if not inexistent) during the night. This suggests that a important variable in the modeling of delays will be take-off time. To check this hypothesis, I look at the behavior of the mean delay as a function of departure time, aggregating the data of the current month:\n",
    "\n",
    "#_______________________________\n",
    "def func2(x, a, b, c):\n",
    "    return a * x**2 +  b*x + c\n",
    "#_______________________________\n",
    "df2['heure_depart'] =  df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\n",
    "test2 = df2['DEPARTURE_DELAY'].groupby(df2['heure_depart']).apply(get_stats).unstack()\n",
    "fct = lambda x:x.hour*3600+x.minute*60+x.second\n",
    "x_val = np.array([fct(s) for s in test2.index]) \n",
    "y_val = test2['mean']\n",
    "popt, pcov = curve_fit(func2, x_val, y_val, p0 = [1, 2, 3])\n",
    "test2['fit'] = pd.Series(func2(x_val, *popt), index = test2.index)\n",
    "which visually gives:\n",
    "\n",
    "fig1 = Figure_style(8, 4, 1, 1)\n",
    "fig1.pos_update(0, 0)\n",
    "fig1.cust_plot_date(df2['heure_depart'], df2['DEPARTURE_DELAY'],\n",
    "                    markeredge=False, label='initial data points')\n",
    "fig1.cust_plot(test2.index, test2['mean'], linestyle='--', linewidth=2, label='mean')\n",
    "fig1.cust_plot(test2.index, test2['fit'], color='r', linestyle='-', linewidth=3, label='fit')\n",
    "fig1.style() ; fig1.draw_legend('upper left')\n",
    "fig1.set_ylabel('Delay (minutes)', fontsize = 14)\n",
    "fig1.set_xlabel('Departure time', fontsize = 14)\n",
    "fig1.set_ylim(-15, 210)\n",
    "\n",
    "Here, we can see that the average delay tends to increase with the departure time of day: flights leave on time in the morning and the delay grows almost monotonously up to 30 minutes at the end of the day. In fact, this behavior is quite general and looking at other aiports or companies, we would find similar trends.\n",
    "\n",
    "6. Predicting flight delays\n",
    "The previsous sections dealt with an exploration of the dataset. Here, I start with the modeling of flight delays. In this section, my goal is to create a model that uses a window of 3 weeks to predict the delays of the following week. Hence, I decide to work on the data of January with the aim of predicting the delays of the epoch  23th−31th  of Januaray\n",
    "\n",
    "df_train = df[df['SCHEDULED_DEPARTURE'].apply(lambda x:x.date()) < datetime.date(2015, 1, 23)]\n",
    "df_test  = df[df['SCHEDULED_DEPARTURE'].apply(lambda x:x.date()) > datetime.date(2015, 1, 23)]\n",
    "df = df_train\n",
    "5.1 Model nº1: one airline, one airport\n",
    "I first decide to model the delays by considering separately the different airlines and by splitting the data according to the different home airports. This first model can be seen as a \"toy-model\" that enables to identify problems that may arise at the production stage. When treating the whole dataset, the number of fits will be large. Hence we have to be sure that the automation of the whole process is robust enough to insure the quality of the fits.\n",
    "\n",
    "5.1.1 Pitfalls \n",
    "a) Unsufficient statistics\n",
    "\n",
    "First of all, I consider the American Airlines flights and make a census of the number of flights that left each airport:\n",
    "\n",
    "carrier = 'AA'\n",
    "check_airports = df[(df['AIRLINE'] == carrier)]['DEPARTURE_DELAY'].groupby(\n",
    "                         df['ORIGIN_AIRPORT']).apply(get_stats).unstack()\n",
    "check_airports.sort_values('count', ascending = False, inplace = True)\n",
    "check_airports[-5:]\n",
    "count\tmax\tmean\tmin\n",
    "ORIGIN_AIRPORT\t\t\t\t\n",
    "JAC\t25.0\t47.0\t-3.640000\t-19.0\n",
    "GUC\t22.0\t199.0\t13.227273\t-24.0\n",
    "SDF\t19.0\t55.0\t8.421053\t-8.0\n",
    "LIT\t9.0\t74.0\t12.555556\t-5.0\n",
    "MTJ\t3.0\t51.0\t26.000000\t-2.0\n",
    "Looking at this list, we can see that the less visited aiports only have a few flights in a month. Thus, in the least favorable case, it is impossible to perform a regression.\n",
    "\n",
    "b) Extreme delays\n",
    "\n",
    "Another pitfall to avoid is that of \"accidental\" delays: a particular attention should be paid to extreme delays. Indeed, during the exploration, it was seen that occasionally, delays of several hours (even tens of hours) could be recorded. This type of delay is however marginal (a few %) and the cause of these delays is probably linked to unpredictable events (weather, breakdown, accident, ...). On the other hand, taking into account a delay of this type will likely introduce a bias in the analysis. Moreover, the weight taken by large values will be significant if we have a small statistics.\n",
    "\n",
    "In order to illustrate this, I first define a function that calculates the mean flights delay per airline and per airport:\n",
    "\n",
    "def get_flight_delays(df, carrier, id_airport, extrem_values = False):\n",
    "    df2 = df[(df['AIRLINE'] == carrier) & (df['ORIGIN_AIRPORT'] == id_airport)]\n",
    "    #_______________________________________\n",
    "    # remove extreme values before fitting\n",
    "    if extrem_values:\n",
    "        df2['DEPARTURE_DELAY'] = df2['DEPARTURE_DELAY'].apply(lambda x:x if x < 60 else np.nan)\n",
    "        df2.dropna(how = 'any')\n",
    "    #__________________________________\n",
    "    # Conversion: date + heure -> heure\n",
    "    df2.sort_values('SCHEDULED_DEPARTURE', inplace = True)\n",
    "    df2['heure_depart'] =  df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\n",
    "    #___________________________________________________________________\n",
    "    # regroupement des vols par heure de départ et calcul de la moyenne\n",
    "    test2 = df2['DEPARTURE_DELAY'].groupby(df2['heure_depart']).apply(get_stats).unstack()\n",
    "    test2.reset_index(inplace=True)\n",
    "    #___________________________________\n",
    "    # conversion de l'heure en secondes\n",
    "    fct = lambda x:x.hour*3600+x.minute*60+x.second\n",
    "    test2.reset_index(inplace=True)\n",
    "    test2['heure_depart_min'] = test2['heure_depart'].apply(fct)\n",
    "    return test2\n",
    "and then a function that performs a linear regression on these values:\n",
    "\n",
    "def linear_regression(test2):\n",
    "    test = test2[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n",
    "    X = np.array(test['heure_depart_min'])\n",
    "    Y = np.array(test['mean'])\n",
    "    X = X.reshape(len(X),1)\n",
    "    Y = Y.reshape(len(Y),1)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X, Y)\n",
    "    result = regr.predict(X)\n",
    "    return X, Y, result\n",
    "I then consider two scenarios. In the first case, I take all the initial values and in the second case, I eliminate all delays greater than 1h before calculating the average delay. The comparison of the two cases is quite explicit:\n",
    "\n",
    "id_airport = 'PHL'\n",
    "df2 = df[(df['AIRLINE'] == carrier) & (df['ORIGIN_AIRPORT'] == id_airport)]\n",
    "df2['heure_depart'] =  df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\n",
    "df2['heure_depart'] = df2['heure_depart'].apply(lambda x:x.hour*3600+x.minute*60+x.second)\n",
    "#___________________\n",
    "# first case\n",
    "test2 = get_flight_delays(df, carrier, id_airport, False)\n",
    "X1, Y1, result2 = linear_regression(test2)\n",
    "#___________________\n",
    "# second case\n",
    "test3 = get_flight_delays(df, carrier, id_airport, True)\n",
    "X2, Y2, result3 = linear_regression(test3)\n",
    "fig1 = Figure_style(8, 4, 1, 1)\n",
    "fig1.pos_update(0, 0)\n",
    "fig1.cust_scatter(df2['heure_depart'], df2['DEPARTURE_DELAY'], markeredge = True)\n",
    "fig1.cust_plot(X1, Y1, color = 'b', linestyle = ':', linewidth = 2, marker = ('b','s', 10))\n",
    "fig1.cust_plot(X2, Y2, color = 'g', linestyle = ':', linewidth = 2, marker = ('g','X', 12))\n",
    "fig1.cust_plot(X1, result2, color = 'b', linewidth = 3)\n",
    "fig1.cust_plot(X2, result3, color = 'g', linewidth = 3)\n",
    "fig1.style()\n",
    "fig1.set_ylabel('Delay (minutes)', fontsize = 14)\n",
    "fig1.set_xlabel('Departure time', fontsize = 14)\n",
    "#____________________________________\n",
    "# convert and set the x ticks labels\n",
    "fct_convert = lambda x: (int(x/3600) , int(divmod(x,3600)[1]/60))\n",
    "fig1.axs[fig1.ix, fig1.iy].set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*fct_convert(x))\n",
    "                                            for x in fig1.axs[fig1.ix, fig1.iy].get_xticks()]);\n",
    "\n",
    "First of all, in this figure, the points corresponding to the individual flights are represented by the points in gray. The mean of these points gives the mean delays and the mean of the set of initial points corresponds to the blue squares. By removing extreme delays (> 1h), one obtains the average delays represented by the green crosses. Thus, in the first case, the fit (solid blue curve) leads to a prediction which corresponds to an average delay of  ∼  10 minutes larger than the predicton obtained in the second case (green curve), and this, at any hour of the day.\n",
    "\n",
    "In conclusion, we see in this example that the way in which we manage the extreme delays will have an important impact on the modeling. Note, however, that the current example corresponds to a chosen case where the impact of extreme delays is magnified by the limited number of flights. Presumably, the impact of such delays will be less pronounced in the majority of cases.\n",
    "\n",
    "5.1.2 Polynomial degree: splitting the dataset\n",
    "In practice, rather than performing a simple linear regression, we can improve the model doing a fit with a polynomial of order  N . Doing so, it is necessary to define the degree  N  which is optimal to represent the data. When increasing the polynomial order, it is important to prevent over-fitting and we do this by splitting the dataset in test and training sets. A problem that may arise with this procedure is that the model ends by indirectly learning the contents of the test set and is thus biased. To avoid this, the data can be re-separated into 3 sets: train, test and validation. An alternative to this technique, which is often more robust, is the so-called cross-validation method. This method consists of performing a first separation of the data in training and test sets. As always, learning is done on the training set, but to avoid over-learning, it is split into several pieces that are used alternately for training and testing.\n",
    "\n",
    "Note that if the data set is small, the separation in test & training sets can introduce a bias in the estimation of the parameters. In practice, the cross-validation method avoids such bias. In fact, in the current model, we will encounter this type of problem and in what follows, I will highlight this. For example, we can consider an extreme case where, after separation, the training set would contain only hours  < 20h and the test set would have hours >  20h. The model would then be unable to reproduce precisely this data, of which it would not have seen equivalent during the training. The cross-validation method avoids this bias because all the data are used successively to drive the model.\n",
    "\n",
    "a) Bias introduced by the separation of the data set\n",
    "\n",
    "In order to test the impact of data separation on model determination, I first define the class fit_polynome :\n",
    "\n",
    "class fit_polynome:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n",
    "\n",
    "    def split(self, method):        \n",
    "        self.method = method        \n",
    "        self.X = np.array(self.data['heure_depart_min'])\n",
    "        self.Y = np.array(self.data['mean'])\n",
    "        self.X = self.X.reshape(len(self.X),1)\n",
    "        self.Y = self.Y.reshape(len(self.Y),1)\n",
    "\n",
    "        if method == 'all':\n",
    "            self.X_train = self.X\n",
    "            self.Y_train = self.Y\n",
    "            self.X_test  = self.X\n",
    "            self.Y_test  = self.Y                        \n",
    "        elif method == 'split':            \n",
    "            self.X_train, self.X_test, self.Y_train, self.Y_test = \\\n",
    "                train_test_split(self.X, self.Y, test_size=0.3)\n",
    "    \n",
    "    def train(self, pol_order):\n",
    "        self.poly = PolynomialFeatures(degree = pol_order)\n",
    "        self.regr = linear_model.LinearRegression()\n",
    "        self.X_ = self.poly.fit_transform(self.X_train)\n",
    "        self.regr.fit(self.X_, self.Y_train)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.X_ = self.poly.fit_transform(X)\n",
    "        self.result = self.regr.predict(self.X_)\n",
    "    \n",
    "    def calc_score(self):        \n",
    "        X_ = self.poly.fit_transform(self.X_test)\n",
    "        result = self.regr.predict(X_)\n",
    "        self.score = metrics.mean_squared_error(result, self.Y_test)\n",
    "The fit_polynome class allows you to perform all operations related to a fit and to save the results. When calling the split() method, the variable 'method' defines how the initial data is separated:\n",
    "\n",
    "method = 'all' : all input data is used to train and then test the model\n",
    "method = 'split' : we use the train_test_split() method of sklearn to define test & training sets\n",
    "Then, the other methods of the class have the following functions:\n",
    "\n",
    "train (n) : drives the data on the training set and makes a polynomial of order n\n",
    "predict (X) : calculates the Y points associated with the X input and for the previously driven model\n",
    "calc_score () : calculates the model score in relation to the test set data\n",
    "In order to illustrate the bias introduced by the selection of the test set, I proceed in the following way: I carry out several \"train / test\" separation of a data set and for each case, I fit polynomials of orders n = 1, 2 and 3 , by calculating their respective scores. Then, I show that according to the choice of separation, the best score can be obtained with any of the values ​​of n . In practice, it is enough to carry out a dozen models to obtain this result. Moreover, this bias is introduced by the choice of the separation \"train / test\" and results from the small size of the dataset to be modeled. In fact, in the following, I take as an example the case of the airline American Airlines (the second biggest airline) and the airport of id 1129804, which is the airport with the most registered flights for that company. This is one of the least favorable scenarios for the emergence of this kind of bias, which, nevertheless, is present:\n",
    "\n",
    "fig = plt.figure(1, figsize=(10,4))\n",
    "\n",
    "ax = ['_' for _ in range(4)]\n",
    "ax[1]=fig.add_subplot(131) \n",
    "ax[2]=fig.add_subplot(132) \n",
    "ax[3]=fig.add_subplot(133) \n",
    "\n",
    "id_airport = 'BNA'\n",
    "test2 = get_flight_delays(df, carrier, id_airport, True)\n",
    "\n",
    "result = ['_' for _ in range(4)]\n",
    "score = [10000 for _ in range(4)]\n",
    "found = [False for _ in range(4)]\n",
    "fit = fit_polynome(test2)\n",
    "\n",
    "color = '.rgbyc'\n",
    "\n",
    "inc = 0\n",
    "while True:\n",
    "    inc += 1\n",
    "    fit.split('split')\n",
    "    for i in range(1,4):\n",
    "        fit.train(pol_order = i)\n",
    "        fit.predict(fit.X)\n",
    "        result[i] = fit.result\n",
    "        fit.calc_score()\n",
    "        score[i]  = fit.score\n",
    "\n",
    "    [ind_min] = [j for j,val in enumerate(score) if min(score) == val]\n",
    "    print(\"modèle nº{:<2}, min. pour n = {}, score = {:.1f}\".format(inc, ind_min,score[ind_min]))\n",
    "    \n",
    "    if not found[ind_min]:            \n",
    "        for i in range(1,4):\n",
    "            ax[ind_min].plot(fit.X, result[i], color[i], linewidth = 4 if i == ind_min else 1)\n",
    "        ax[ind_min].scatter(fit.X, fit.Y)                \n",
    "        ax[ind_min].text(0.05, 0.95, 'MSE = {:.1f}, {:.1f}, {:.1f}'.format(*score[1:4]),\n",
    "                         style='italic', transform=ax[ind_min].transAxes, fontsize = 8,\n",
    "                         bbox={'facecolor':'tomato', 'alpha':0.8, 'pad':5})                \n",
    "        found[ind_min] = True\n",
    "\n",
    "    shift = 0.5\n",
    "    plt.text(-1+shift, 1.05, \"polynomial order:\", color = 'k',\n",
    "                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n",
    "    plt.text(0+shift, 1.05, \"n = 1\", color = 'r', \n",
    "                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n",
    "    plt.text(0.4+shift, 1.05, \"n = 2\", color = 'g', \n",
    "                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n",
    "    plt.text(0.8+shift, 1.05, \"n = 3\", color = 'b',\n",
    "                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n",
    "   \n",
    "    if inc == 40 or all(found[1:4]): break\n",
    "modèle nº1 , min. pour n = 1, score = 61.0\n",
    "modèle nº2 , min. pour n = 1, score = 16.1\n",
    "modèle nº3 , min. pour n = 1, score = 174.1\n",
    "modèle nº4 , min. pour n = 2, score = 127.6\n",
    "modèle nº5 , min. pour n = 3, score = 11.8\n",
    "\n",
    "In this figure, the panels from left to right correspond to 3 separations of the data in train and test sets, for which the best models are obtained respectively with polynomials of order 1, 2 and 3. On each of these panels the 3 fits polynomials have been represented and the best model corresponds to the thick curve.\n",
    "\n",
    "b) Selection by cross-validation\n",
    "\n",
    "One of the advantages of the cross-validation method is that it avoids the bias that has just been put forward when choosing the polynomial degree. In order to use this method, I define a new class that I will use later to perform the fits:\n",
    "\n",
    "class fit_polynome_cv:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n",
    "        self.X = np.array(self.data['heure_depart_min'])\n",
    "        self.Y = np.array(self.data['mean'])\n",
    "        self.X = self.X.reshape(len(self.X),1)\n",
    "        self.Y = self.Y.reshape(len(self.Y),1)\n",
    "\n",
    "    def train(self, pol_order, nb_folds):\n",
    "        self.poly = PolynomialFeatures(degree = pol_order)\n",
    "        self.regr = linear_model.LinearRegression()\n",
    "        self.X_ = self.poly.fit_transform(self.X)\n",
    "        self.result = cross_val_predict(self.regr, self.X_, self.Y, cv = nb_folds)\n",
    "    \n",
    "    def calc_score(self, pol_order, nb_folds):\n",
    "        self.poly = PolynomialFeatures(degree = pol_order)\n",
    "        self.regr = linear_model.LinearRegression()\n",
    "        self.X_ = self.poly.fit_transform(self.X)\n",
    "        self.score = np.mean(cross_val_score(self.regr, self.X_, self.Y,\n",
    "                                             cv = nb_folds, scoring = 'mean_squared_error'))\n",
    "This class has two methods:\n",
    "\n",
    "train (n, nb_folds) : defined 'nb_folds' training sets from the initial dataset and drives a 'n' order polynomial on each of these sets. This method returns as a result the Y predictions obtained for the different test sets.\n",
    "calc_score (n, nb_folds) : performs the same procedure as a train method except that this method calculates the fit score and not the predicted values ​​on the different test data.\n",
    "By default, the 'K-fold' method is used by sklearn cross_val_predict () and cross_val_score () methods. These methods are deterministic in the choice of the K folds, which implies that for a fixed K value, the results obtained using these methods will always be identical. As seen in the previous example, this was not the case when using the train_test_split() method. Thus, if we take the same dataset as in the previous example, the method of cross validation makes it possible to choose the best polynomial degree:\n",
    "\n",
    "Max possible number of folds: 16 \n",
    "\n",
    "n=1 -> MSE = 130.629\n",
    "n=2 -> MSE = 151.79\n",
    "n=3 -> MSE = 159.455\n",
    "n=4 -> MSE = 162.631\n",
    "n=5 -> MSE = 166.966\n",
    "n=6 -> MSE = 173.08\n",
    "n=7 -> MSE = 181.361\n",
    "We can see that using this method gives us that the best model (ie the best generalized model) is obtained with a polynomial of order 2. At this stage of the procedure, the choice of the polynomial order a has been validated and we can now use all the data in order to perform the fit:\n",
    "\n",
    "Thus, in the following figure, the juxtaposition of the K = 50 polynomial fits corresponding to the cross validation calculation leads to the red curve. The polynomial fit corresponding to the final model corresponds to the blue curve.\n",
    "\n",
    "fit2.train(pol_order = 2, nb_folds = nb_folds)\n",
    "fig1 = Figure_style(8, 4, 1, 1) ; fig1.pos_update(0, 0)\n",
    "fig1.cust_scatter(fit2.X, fit2.Y, markeredge = True, label = 'initial data points')\n",
    "fig1.cust_plot(fit.X,fit2.result,color=u'#1f77b4',linestyle='--',linewidth=2,label='CV output')\n",
    "fig1.cust_plot(fit.X,fit.result,color=u'#ff7f0e',linewidth = 3,label='final fit')\n",
    "fig1.style(); fig1.draw_legend('upper left')\n",
    "fig1.set_ylabel('Delay (minutes)') ; fig1.set_xlabel('Departure time')\n",
    "#____________________________________\n",
    "# convert and set the x ticks labels\n",
    "fct_convert = lambda x: (int(x/3600) , int(divmod(x,3600)[1]/60))\n",
    "fig1.axs[fig1.ix, fig1.iy].set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*fct_convert(x))\n",
    "                                            for x in fig1.axs[fig1.ix, fig1.iy].get_xticks()]);\n",
    "\n",
    "56.862847718920953\n",
    "5.1.3 Model test: prediction of end-January delays\n",
    "At this stage, the model was driven is tested on the training set which include the data of the first 3 weeks of January. We now look at the comparison of predictions and observations for the fourth week of January:\n",
    "\n",
    "test_data = get_flight_delays(df_test, carrier, id_airport, True)\n",
    "test_data = test_data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n",
    "X_test = np.array(test_data['heure_depart_min'])\n",
    "Y_test = np.array(test_data['mean'])\n",
    "X_test = X_test.reshape(len(X_test),1)\n",
    "Y_test = Y_test.reshape(len(Y_test),1)\n",
    "fit.predict(X_test)\n",
    "and the MSE score of the model is:\n",
    "\n",
    "score = metrics.mean_squared_error(fit.result, Y_test)\n",
    "score\n",
    "108.67130851577079\n",
    "To get an idea of the meaning of such a value for the MSE, we can assume a constant error on each point of the dataset. In which case, at each point  i , we have:\n",
    "\n",
    "yi−f(xi)=cste=MSE‾‾‾‾‾√\n",
    " \n",
    "thus giving the difference in minutes between the predicted delay and the actual delay. In this case, the difference between the model and the observations is thus typically:\n",
    "\n",
    "'Ecart = 10.42 min'\n",
    "5.2 Model nº2: One airline, all airports\n",
    "In the previous section, the model only considered one airport. This procedure is potentially inefficient because it is likely that some of the observations can be extrapolated from an ariport to another. Thus, it may be advantageous to make a single fit, which would take all the airports into account. In particular, this will allow to predict delays on airports for which the number of data is low with a better accuracy.\n",
    "\n",
    "(1831, 3)\n",
    "In the merged_df dataframe, airports are referenced by an identifier given in the ORIGIN_AIRPORT variable. The corresponding labels can't be used directly in a fit and I thus use the one-hot-encoding method:\n",
    "\n",
    "[(0, 'ABQ'), (1, 'ATL'), (2, 'AUS'), (3, 'BDL'), (4, 'BHM')]\n",
    "Above, I have assigned a label to each airport. The correspondence between the label and the original identifier has been saved in the label_airport list. Now I proceed with the \"One Hot Encoding\" by creating a matrix where instead of the ORIGIN_AIRPORT variable that contained  M  labels, we build a matrix with  M  columns, filled of 0 and 1 depending on the correspondance with particular airports:\n",
    "\n",
    "(1831, 82) (1831, 1)\n",
    "5.2.1 Linear regression\n",
    "The matrices X and Y thus created can be used to perform a linear regression:\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X,Y)\n",
    "predictions = lm.predict(X)\n",
    "print(\"MSE =\", metrics.mean_squared_error(predictions, Y))\n",
    "MSE = 53.7430736542\n",
    "Here, I calculated the MSE score of the fit. In practice, we can have a feeling of the quality of the fit by considering the number of predictions where the differences with real values is greater than 15 minutes:\n",
    "\n",
    "icount = 0\n",
    "for i, val in enumerate(Y):\n",
    "    if abs(val-predictions[i]) > 15: icount += 1\n",
    "'{:.2f}%'.format(icount / len(predictions) * 100)\n",
    "'5.30%'\n",
    "In practice, this model tends to underestimate the large delays, which can be seen in the following figure:\n",
    "\n",
    "tips = pd.DataFrame()\n",
    "tips[\"prediction\"] = pd.Series([float(s) for s in predictions]) \n",
    "tips[\"original_data\"] = pd.Series([float(s) for s in Y]) \n",
    "sns.jointplot(x=\"original_data\", y=\"prediction\", data=tips, size = 6, ratio = 7,\n",
    "              joint_kws={'line_kws':{'color':'limegreen'}}, kind='reg')\n",
    "plt.xlabel('Mean delays (min)', fontsize = 15)\n",
    "plt.ylabel('Predictions (min)', fontsize = 15)\n",
    "plt.plot(list(range(-10,25)), list(range(-10,25)), linestyle = ':', color = 'r')\n",
    "sns.plt.show()\n",
    "\n",
    "5.2.2 Polynomial regression\n",
    "I will now extend the previous fit by using a polynomial rather than a linear function:\n",
    "\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "MSE = 49.5025438214\n",
    "We can see that a polynomial fit improves slightly the MSE score. In practice, the percentage of values where the difference between predictions and real delays is greater than 15 minutes is:\n",
    "\n",
    "'4.81%'\n",
    "And as before, it can be seen that model tends to be worse in the case of large delays:\n",
    "\n",
    "\n",
    "5.2.3 Setting the free parameters\n",
    "Above, the two models were fit and tested on the training set. In practice, as mentioned above, there is a risk of overfitting by proceeding that way and the free parameters of the model will be biased. Hence, the model will not allow a good generalization. In what follows, I will therefore split the datas in order to train and then test the model. The purpose will be to determine the polynomial degree which allows the best generalization of the predictions:\n",
    "\n",
    "As before, I fit the model on the training set:\n",
    "\n",
    "(1281, 82)\n",
    "Mean squared error =  46.3925583884\n",
    "Now, by testing on the test set we get:\n",
    "\n",
    "Mean squared error =  208151.428081\n",
    "Here, we see that the fit is particularly bad with a MSE > 500 (the exact value depends on the run and on the splitting of the dataset), which means that the fit performs poorly when generalyzing to other data. Now let's examine in detail the reasons why we have such a bad score. Below, I examing all the terms of the MSE calculation and identify the largest terms:\n",
    "\n",
    "somme = 0\n",
    "for valeurs in zip(result, Y_test):\n",
    "    ajout = (float(valeurs[0]) - float(valeurs[1]))**2\n",
    "    somme += ajout\n",
    "    if ajout > 10**4:\n",
    "        print(\"{:<.1f} {:<.1f} {:<.1f}\".format(ajout, float(valeurs[0]), float(valeurs[1])))\n",
    "253024.0 -503.0 0.0\n",
    "55989950.2 7508.6 26.0\n",
    "57930603.6 7615.6 4.4\n",
    "262655.8 -506.3 6.2\n",
    "We see that some predictions show very large errors. In practice, this can be explained by the fact that during the separation in train and test sets, data with no equivalent in the training set was put in the test data. Thus, when calculating the prediction, the model has to perform an extrapolation. If the coefficients of the fit are large (which is often the case when overfitting), extrapolated values will show important values, as in the present case. In order to have a control over this phenomenon, we can use a regularization method which will put a penalty to the models whose coefficients are the most important:\n",
    "\n",
    "Ridge(alpha=0.3, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "   normalize=True, random_state=None, solver='auto', tol=0.001)\n",
    "Now, if we calculate the score associated to the predictions made with a regularization technique, we have:\n",
    "\n",
    "Mean squared error =  65.4841834479\n",
    "And we can see that we obtain a reasonnable score. Hence, with the current procedure, to determine the best model, we have two free parameters to adjust: the polynomial order and the  α  coefficient of the 'Ridge Regression' :\n",
    "\n",
    "n=1 alpha=0 , MSE = 65.29\n",
    "n=1 alpha=2 , MSE = 64.424\n",
    "n=1 alpha=4 , MSE = 64.29\n",
    "n=1 alpha=6 , MSE = 64.458\n",
    "n=1 alpha=8 , MSE = 64.75\n",
    "n=1 alpha=10 , MSE = 65.09\n",
    "n=1 alpha=12 , MSE = 65.439\n",
    "n=1 alpha=14 , MSE = 65.78\n",
    "n=1 alpha=16 , MSE = 66.105\n",
    "n=1 alpha=18 , MSE = 66.411\n",
    "n=2 alpha=0 , MSE = 9.6708e+17\n",
    "n=2 alpha=2 , MSE = 65.612\n",
    "n=2 alpha=4 , MSE = 65.396\n",
    "n=2 alpha=6 , MSE = 65.285\n",
    "n=2 alpha=8 , MSE = 65.236\n",
    "n=2 alpha=10 , MSE = 65.235\n",
    "n=2 alpha=12 , MSE = 65.27\n",
    "n=2 alpha=14 , MSE = 65.33\n",
    "n=2 alpha=16 , MSE = 65.409\n",
    "n=2 alpha=18 , MSE = 65.501\n",
    "This grid search allows to find the best set of  α  and  n  parameters. Let us note, however, that for this model, the estimates obtained with a linear regression or a polynomial of order 2 are quite close. Now I use these parameters to test this template over the test set:\n",
    "\n",
    "54.9920975427\n",
    "6.2.4 Testing the model: delays of end-january\n",
    "At this stage, model predictions are tested against end-January data. These data are first extracted:\n",
    "\n",
    "carrier = 'AA'\n",
    "merged_df_test = get_merged_delays(df_test, carrier)\n",
    "then I convert them into a format suitable to perform the fit. At this stage, I manually do one-hot-encoding by re-using the labeling that had been established on the training data:\n",
    "\n",
    "label_conversion = dict()\n",
    "for s in label_airports:\n",
    "    label_conversion[s[1]] = s[0]\n",
    "\n",
    "merged_df_test['AIRPORT_ID'].replace(label_conversion, inplace = True)\n",
    "\n",
    "for index, label in label_airports:\n",
    "    temp = merged_df_test['AIRPORT_ID'] == index\n",
    "    temp = temp.apply(lambda x:1.0 if x else 0.0)\n",
    "    if index == 0:\n",
    "        matrix = np.array(temp)\n",
    "    else:\n",
    "        matrix = np.vstack((matrix, temp))\n",
    "matrix = matrix.T\n",
    "\n",
    "b = np.array(merged_df_test['heure_depart_min'])\n",
    "b = b.reshape(len(b),1)\n",
    "X_test = np.hstack((matrix, b))\n",
    "Y_test = np.array(merged_df_test['mean'])\n",
    "Y_test = Y_test.reshape(len(Y_test), 1)\n",
    "I can then create the predictions\n",
    "\n",
    "X_ = poly.fit_transform(X_test)\n",
    "result = ridgereg.predict(X_)\n",
    "score = metrics.mean_squared_error(result, Y_test)\n",
    "'MSE = {:.2f}'.format(score)\n",
    "'MSE = 59.36'\n",
    "As before, assuming that the delay is independent of the point, this MSE score is equivalent to an average delay of:\n",
    "\n",
    "'Ecart = {:.2f} min'.format(np.sqrt(score))\n",
    "'Ecart = 7.70 min'\n",
    "The current MSE score is calculated on all the airports served by American Airlines, whereas previously it was calculated on the data of a single airport. The current model is therefore more general. Moreover, considering the previous model, it is likely that predictions will be poor for airports with low statistics.\n",
    "\n",
    "6.3 Model nº3: Accounting for destinations\n",
    "In the previous model, I grouped the flights per departure time. Thus, flights with different destinations were grouped as soon as they leave at the same time. Now I make a model that accounts for both departure and arrival times:\n",
    "\n",
    "def create_df(df, carrier):\n",
    "    df2 = df[df['AIRLINE'] == carrier][['SCHEDULED_DEPARTURE','SCHEDULED_ARRIVAL',\n",
    "                                    'ORIGIN_AIRPORT','DESTINATION_AIRPORT','DEPARTURE_DELAY']]\n",
    "    df2.dropna(how = 'any', inplace = True)\n",
    "    df2['weekday'] = df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.weekday())\n",
    "    #____________________\n",
    "    # delete delays > 1h\n",
    "    df2['DEPARTURE_DELAY'] = df2['DEPARTURE_DELAY'].apply(lambda x:x if x < 60 else np.nan)\n",
    "    df2.dropna(how = 'any', inplace = True)\n",
    "    #_________________\n",
    "    # formating times\n",
    "    fct = lambda x:x.hour*3600+x.minute*60+x.second\n",
    "    df2['heure_depart'] = df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\n",
    "    df2['heure_depart'] = df2['heure_depart'].apply(fct)\n",
    "    df2['heure_arrivee'] = df2['SCHEDULED_ARRIVAL'].apply(fct)\n",
    "    df3 = df2.groupby(['heure_depart', 'heure_arrivee', 'ORIGIN_AIRPORT'],\n",
    "                      as_index = False).mean()\n",
    "    return df3\n",
    "heure_depart\theure_arrivee\tORIGIN_AIRPORT\tDEPARTURE_DELAY\tweekday\n",
    "0\t300\t17640\tLAX\t2.133333\t2.800000\n",
    "1\t300\t17700\tLAX\t5.500000\t3.750000\n",
    "2\t600\t28200\tLAX\t-6.000000\t3.250000\n",
    "3\t1200\t29040\tLAX\t-4.117647\t2.823529\n",
    "4\t1200\t29100\tLAX\t0.800000\t3.600000\n",
    "Henceforth, regroupings are made on departure and arrival times, and the (specific) airports of origin and destination are implicitly taken into account. As before, I carry out the encoding of the airports:\n",
    "\n",
    "6.3.1 Choice of model parameters\n",
    "As before, I will perform a regression with regularization and I will have to define the value to attribute to the parameter  α . I therefore separate the data to train and then test the model to select the best value for  α :\n",
    "\n",
    "n=1 alpha=0.0 , MSE = 85.599\n",
    "n=1 alpha=0.2 , MSE = 84.456\n",
    "n=1 alpha=0.4 , MSE = 84.313\n",
    "n=1 alpha=0.6 , MSE = 84.598\n",
    "n=1 alpha=0.8 , MSE = 85.082\n",
    "n=1 alpha=1.0 , MSE = 85.655\n",
    "n=1 alpha=1.2 , MSE = 86.26\n",
    "n=1 alpha=1.4 , MSE = 86.867\n",
    "n=1 alpha=1.6 , MSE = 87.46\n",
    "n=1 alpha=1.8 , MSE = 88.03\n",
    "n=2 alpha=0.0 , MSE = 9.4162e+12\n",
    "n=2 alpha=0.2 , MSE = 86.377\n",
    "n=2 alpha=0.4 , MSE = 85.564\n",
    "n=2 alpha=0.6 , MSE = 85.185\n",
    "n=2 alpha=0.8 , MSE = 84.98\n",
    "n=2 alpha=1.0 , MSE = 84.874\n",
    "n=2 alpha=1.2 , MSE = 84.837\n",
    "n=2 alpha=1.4 , MSE = 84.85\n",
    "n=2 alpha=1.6 , MSE = 84.901\n",
    "n=2 alpha=1.8 , MSE = 84.982\n",
    "89.5503177274\n",
    "6.3.2 Test of the model: late January delays\n",
    "Now I test the quality of the predictions on the data of the last week of January. I first extract these data:\n",
    "\n",
    "heure_depart\theure_arrivee\tORIGIN_AIRPORT\tDEPARTURE_DELAY\tweekday\n",
    "0\t300\t17640\tLAX\t-4.000\t3.25\n",
    "1\t1200\t29040\tLAX\t5.125\t3.25\n",
    "2\t1800\t20340\tSFO\t-6.750\t3.25\n",
    "3\t2700\t29340\tLAS\t-4.500\t3.25\n",
    "4\t3900\t31800\tLAX\t-4.875\t3.25\n",
    "MSE = 74.8\n",
    "which corresponds to an average delay of:\n",
    "\n",
    "'Ecart = 8.65 min'\n",
    "ecarts > 15 minutes: 4.588%\n",
    "\n",
    "Conclusion\n",
    "These notebook was two-fold. The first part dealt with an exploration of the dataset, with the aim of understanding some properties of the delays registered by flights. This exploration gave me the occasion of using various vizualization tools offered by python. The second part of the notebook consisted in the elaboration of a model aimed at predicting flight delays. For that purpose, I used polynomial regressions and showed the importance of regularisation techniques. In fact, I only used ridge regression but it is important to keep in mind that other regularisations techniques could be more appropriate ( e.g Lasso or Elastic net).\n",
    "\n",
    "If you see any kind of improvement, or mistakes, thanks in advance for telling me !! \n",
    "If you liked this notebook, thanks for upvoting :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
